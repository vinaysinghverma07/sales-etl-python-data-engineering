# Sales Data ETL Pipeline using Python ###

## ğŸ“Œ Overview

This project implements an end-to-end ETL (Extract, Transform, Load) pipeline using Python, following production-grade data engineering best practices.

## The pipeline:
1. Extracts raw sales data from CSV.
2. Validates and cleans the data.
3. Applies business transformations.
4. Loads the processed data into a structured output folder.
5. Implements centralized logging, configuration management, and error handling.


This project is designed to simulate real-world data engineering workflows, not just scripting.

## ğŸ—ï¸ Architecture Overview:

    Raw CSV
    â”‚
    â–¼
    Extract Module
    â”‚
    â–¼
    Transform Module
    â”‚
    â–¼
    Load Module
    â”‚
    â–¼
    Processed CSV + Logs

## Key architectural principles:
  1. Config-driven design
  2. Centralized logging
  3. Modular codebase
  4. Production-safe validations

## ğŸ“‚ Project Structure:

    sales_etl/
    â”‚
    â”œâ”€â”€ config/
    â”‚   â””â”€â”€ config.yaml          # Configuration file (paths, logging)
    â”‚
    â”œâ”€â”€ data/
    â”‚   â”œâ”€â”€ raw/
    â”‚   â”‚   â””â”€â”€ sales_data.csv   # Input raw data
    â”‚   â””â”€â”€ processed/
    â”‚       â””â”€â”€ sales_processed_<timestamp>.csv
    â”‚
    â”œâ”€â”€ logs/
    â”‚   â””â”€â”€ etl.log              # Centralized ETL logs
    â”‚
    â”œâ”€â”€ src/
    â”‚   â”œâ”€â”€ __init__.py          # Marks src as a Python package
    â”‚   â”œâ”€â”€ utils.py             # Config loading & logging setup
    â”‚   â”œâ”€â”€ extract.py           # Data extraction & schema validation
    â”‚   â”œâ”€â”€ transform.py         # Data cleaning & transformations
    â”‚   â”œâ”€â”€ load.py              # Data loading logic
    â”‚   â””â”€â”€ testig.py            # Entry point for pipeline execution
    â”‚
    â””â”€â”€ README.md


## âš™ï¸ Technologies & Libraries Used

| # | Library | Purpose |
|---|--------|---------|
| 1 | `pandas` | Data manipulation and transformation |
| 2 | `yaml` (PyYAML) | Reading configuration files |
| 3 | `logging` | Application-level logging |
| 4 | `pathlib` | OS-independent path handling |
| 5 | `datetime` | Timestamped outputs |
| 6 | Python 3.10+ | Language runtime |

## ğŸ”§ Configuration Management (config/config.yaml):
  All runtime configurations are externalized:
    
    raw_data_path: data/raw/sales_data.csv
    processed_data_path: data/processed
    log_file_path: logs/etl.log

    logging:
      level: INFO


### Why configuration files?:
  
  1. No hardcoded paths
  2. Easy environment changes
  3. Production best practice

## ğŸ§° Module Breakdown

### ğŸ”¹ `utils.py` â€“ Shared Utilities

**Purpose**
- Load configuration from YAML
- Setup centralized logging
- Resolve project root dynamically

**Key Functions**

- **`load_config()`**
  - Reads `config.yaml`
  - Returns configuration as a Python dictionary

- **`setup_logging(log_file_path, level)`**
  - Creates log directory if missing
  - Logs to both file & console
  - Returns a reusable logger instance

ğŸ“Œ *Used by all ETL modules*

---

### ğŸ”¹ `extract.py` â€“ Data Extraction

**Purpose**
- Read raw CSV safely
- Normalize malformed CSV structures
- Validate schema before processing

**Key Responsibilities**
- Detect incorrect delimiters
- Fix malformed headers
- Validate required columns
- Log extraction metrics (rows, columns, schema issues)

**Key Libraries Used**
- `pandas.read_csv`
- `pathlib.Path`

---

### ğŸ”¹ `transform.py` â€“ Data Transformation

**Purpose**
- Clean and standardize raw data
- Apply business logic transformations

**Transformations Implemented**

| Issue | Fix |
|----|----|
| Missing quantity | Filled with `1` |
| Duplicate `order_id` | Removed |
| String dates | Converted to `datetime` |
| String numbers | Converted to numeric |
| Missing revenue | Derived column |

**Key Concepts Used**
- `pandas.to_numeric`
- `pandas.to_datetime`
- Vectorized operations
- Defensive programming

---

### ğŸ”¹ `load.py` â€“ Data Loading

**Purpose**
- Persist processed data safely
- Prevent overwrites
- Support auditability

**Key Features**
- Auto-create output directory
- Timestamped output files
- Final schema validation
- Exception logging

**Example Output**
    sales_processed_20260112_175609.csv


---

### ğŸ”¹ `testig.py` â€“ Pipeline Entry Point

**Purpose**
- Orchestrate the ETL pipeline
- Centralized error handling
- Single execution point

**Key Pattern Used**
```python
try:
    extract â†’ transform â†’ load
except Exception:
    log error + fail pipeline
```
ğŸ“Œ This mimics Airflow / Databricks driver behavior

---
## ğŸªµ Logging Strategy

**Logs written to**
- Console (real-time)
- `logs/etl.log` (persistent)

**Logging Characteristics**
- Uses `logger.exception()` for full stack traces
- Centralized logging setup (configured once per run)

**Sample Log Entry**
``` 2026-01-12 17:56:09 - INFO - Starting data extraction step ```


---

## â— Error Handling Strategy

| Scenario | Handling |
|--------|----------|
| Missing file | Logged + raised |
| Schema mismatch | Logged + pipeline stopped |
| Empty DataFrame | Logged + prevented load |
| Permission issues | Logged with stack trace |

ğŸ“Œ Errors are logged at the **entry point**, not swallowed inside individual functions.

---

## ğŸš€ How to Run the Project

```bash
python src/testig.py
```

**Ensure**
- Python 3.10+
- Required libraries installed
- config.yaml paths are correct

ğŸ“ˆ What This Project Demonstrates

âœ… Real-world ETL design
âœ… Production logging patterns
âœ… Config-driven pipelines
âœ… Defensive data engineering
âœ… GitHub-ready project structure

## ğŸ§­ Next Enhancements (Planned)
- Incremental load logic
- PostgreSQL & MS SQL loads
- Pytest unit tests
- Data quality checks
- Airflow / ADF orchestration
- Databricks PySpark migration
